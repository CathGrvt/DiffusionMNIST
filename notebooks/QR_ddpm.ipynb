{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50FGtZsk1y-b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d16i_bcV1y-d"
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        *,\n",
    "        expected_shape,\n",
    "        act=nn.GELU,\n",
    "        kernel_size=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.LayerNorm((out_channels, *expected_shape)),\n",
    "            act(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSvzdt1f1y-d"
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        expected_shape=(28, 28),\n",
    "        n_hidden=(64, 128, 64),\n",
    "        kernel_size=7,\n",
    "        last_kernel_size=3,\n",
    "        time_embeddings=16,\n",
    "        act=nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        last = in_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for hidden in n_hidden:\n",
    "            self.blocks.append(\n",
    "                CNNBlock(\n",
    "                    last,\n",
    "                    hidden,\n",
    "                    expected_shape=expected_shape,\n",
    "                    kernel_size=kernel_size,\n",
    "                    act=act,\n",
    "                )\n",
    "            )\n",
    "            last = hidden\n",
    "\n",
    "        # The final layer, we use a regular Conv2d to get the\n",
    "        # correct scale and shape (and avoid applying the activation)\n",
    "        self.blocks.append(\n",
    "            nn.Conv2d(\n",
    "                last,\n",
    "                in_channels,\n",
    "                last_kernel_size,\n",
    "                padding=last_kernel_size // 2,\n",
    "            )\n",
    "        )\n",
    "        # final_conv = nn.Conv2d(\n",
    "        #     last,\n",
    "        #     in_channels,\n",
    "        #     last_kernel_size,\n",
    "        #     padding=last_kernel_size // 2,\n",
    "        # )\n",
    "        # # Use Xavier initialization for the final convolutional layer\n",
    "        # init.xavier_uniform_(final_conv.weight)\n",
    "        # init.constant_(final_conv.bias, 0)\n",
    "        # self.blocks.append(final_conv)\n",
    "\n",
    "        ## This part is literally just to put the single scalar \"t\" into the CNN\n",
    "        ## in a nice, high-dimensional way:\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_embeddings * 2, 128),\n",
    "            act(),\n",
    "            nn.Linear(128, 128),\n",
    "            act(),\n",
    "            nn.Linear(128, 128),\n",
    "            act(),\n",
    "            nn.Linear(128, n_hidden[0]),\n",
    "        )\n",
    "        frequencies = torch.tensor(\n",
    "            [0] + [2 * np.pi * 1.5**i for i in range(time_embeddings - 1)]\n",
    "        )\n",
    "        self.register_buffer(\"frequencies\", frequencies)\n",
    "\n",
    "    def time_encoding(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        phases = torch.concat(\n",
    "            (\n",
    "                torch.sin(t[:, None] * self.frequencies[None, :]),\n",
    "                torch.cos(t[:, None] * self.frequencies[None, :]) - 1,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return self.time_embed(phases)[:, :, None, None]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # Shapes of input:\n",
    "        #    x: (batch, chan, height, width)\n",
    "        #    t: (batch,)\n",
    "\n",
    "        embed = self.blocks[0](x)\n",
    "        # ^ (batch, n_hidden[0], height, width)\n",
    "\n",
    "        # Add information about time along the diffusion process\n",
    "        #  (Providing this information by superimposing in latent space)\n",
    "        embed += self.time_encoding(t)\n",
    "        #         ^ (batch, n_hidden[0], 1, 1) - thus, broadcasting\n",
    "        #           to the entire spatial domain\n",
    "\n",
    "        for block in self.blocks[1:]:\n",
    "            embed = block(embed)\n",
    "\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fKx7CLAtdV_"
   },
   "outputs": [],
   "source": [
    "def create_sigma_schedule(n_T):\n",
    "    # Sigmoid schedule from 0 to 1 over n_T steps\n",
    "    timesteps = torch.linspace(-4, 6, steps=n_T)  # Wide range for a gradual sigmoid\n",
    "    # maybe change to 6\n",
    "    sigma_schedule = torch.sigmoid(\n",
    "        timesteps\n",
    "    )  # Sigmoid function for non-linear scheduling\n",
    "    return sigma_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCZe8Q651y-d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gt,\n",
    "        n_T: int,\n",
    "        criterion: nn.Module = nn.MSELoss(),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.gt = gt\n",
    "        self.n_T = n_T\n",
    "\n",
    "        noise_schedule = create_sigma_schedule(n_T)\n",
    "        self.register_buffer(\"sigma_t\", noise_schedule)\n",
    "        self.sigma_t\n",
    "\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def apply_qr_transformation(self, x, intensity):\n",
    "        \"\"\"Apply a QR code pattern onto an image with a given intensity.\"\"\"\n",
    "        # x is a batch of images of shape (B, C, H, W)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        qr_codes = self.create_batch_random_qr(batch_size, (channels, height, width))\n",
    "        # Make sure intensity is broadcastable over the batch\n",
    "        intensity = intensity.view(-1, 1, 1, 1).to(x.device)\n",
    "        qr_codes = qr_codes.to(x.device)\n",
    "        # Apply the QR code pattern with the given intensity\n",
    "        transformed_x = x * (1 - intensity) + qr_codes * (intensity)\n",
    "\n",
    "        return transformed_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample a random time step t for each batch element\n",
    "        t = torch.randint(0, self.n_T, (x.shape[0],), device=x.device)\n",
    "\n",
    "        # Get the sigma for the QR code transformation at the corresponding time step t\n",
    "        sigma_t = self.sigma_t[t]\n",
    "\n",
    "        # Transform each image in the batch x with the corresponding QR code pattern\n",
    "        z_t = self.apply_qr_transformation(x, sigma_t)\n",
    "\n",
    "        # The CNN tries to predict the original image x from the QR-coded image z_t\n",
    "        preds = self.gt(z_t, t / self.n_T)\n",
    "\n",
    "        # Return the loss between the original images x and the predictions preds\n",
    "        loss = self.criterion(x, preds)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def create_batch_random_qr(self, batch_size, image_size):\n",
    "        channels, height, width = image_size\n",
    "        # Initialize an empty list to store the pseudo-QR images\n",
    "        pseudo_qr_batch_list = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            # Create a random binary pattern\n",
    "            pseudo_qr_array = np.random.choice(\n",
    "                [0, 255], size=(height, width), p=[0.5, 0.5]\n",
    "            ).astype(np.uint8)\n",
    "\n",
    "            # If more than one channel is needed, replicate the grayscale pattern across the channels\n",
    "            if channels > 1:\n",
    "                pseudo_qr_array = np.repeat(\n",
    "                    pseudo_qr_array[..., np.newaxis], channels, axis=-1\n",
    "                )\n",
    "\n",
    "            pseudo_qr_batch_list.append(pseudo_qr_array)\n",
    "\n",
    "        # Convert the list of arrays into a single NumPy array\n",
    "        pseudo_qr_batch = np.stack(pseudo_qr_batch_list, axis=0)\n",
    "\n",
    "        # Convert the NumPy array to a PyTorch tensor and normalize to [0, 1]\n",
    "        pseudo_qr_batch_tensor = torch.from_numpy(pseudo_qr_batch).float() / 255.0\n",
    "\n",
    "        return pseudo_qr_batch_tensor.unsqueeze(\n",
    "            1\n",
    "        )  # Add a channel dimension, resulting in shape (B, 1, H, W)\n",
    "\n",
    "    def sample(self, n_sample, size, device):\n",
    "        # Start with a fully 'QR-coded' random image\n",
    "        z_t = self.create_batch_random_qr(n_sample, size).to(device)\n",
    "        _one = torch.ones(n_sample, device=device)\n",
    "        for t in reversed(range(0, self.n_T)):\n",
    "            if t > 0:\n",
    "                sigma_t = self.sigma_t[t]\n",
    "                sigma_t_minus_1 = self.sigma_t[t - 1]\n",
    "\n",
    "                # Here we generate the predictions from the model, which are presumably 'less QR-coded' than z_t\n",
    "                x_0_pred = self.gt(z_t, (t / self.n_T) * _one)\n",
    "\n",
    "                # Undo the QR code transformation by a schedule, gradually revealing the MNIST image\n",
    "                z_t = (\n",
    "                    z_t\n",
    "                    - self.apply_qr_transformation(x_0_pred, sigma_t)\n",
    "                    + self.apply_qr_transformation(x_0_pred, sigma_t_minus_1)\n",
    "                )\n",
    "            else:\n",
    "                # The final step should be the MNIST image with no QR code pattern applied\n",
    "                z_t = x_0_pred\n",
    "\n",
    "        return z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6jMrCRa1y-d"
   },
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6ApENps1y-d"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "gt = CNN(\n",
    "    in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 64, 32, 16), act=nn.GELU\n",
    ")\n",
    "# For testing: (16, 32, 32, 16)\n",
    "# For more capacity (for example): (64, 128, 256, 128, 64)\n",
    "ddpm = DDPM(gt=gt, n_T=1000)\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=30, gamma=0.1)\n",
    "# Initialise the scheduler with a gamma close to 1 for slow decay:\n",
    "# scheduler = ExponentialLR(optim, gamma=0.99)\n",
    "scheduler = ReduceLROnPlateau(optim, mode=\"min\", factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqySzi92u0Q9"
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "# We wrap our model, optimizer, and dataloaders with `accelerator.prepare`,\n",
    "# which lets HuggingFace's Accelerate handle the device placement and gradient accumulation.\n",
    "ddpm, optim, dataloader = accelerator.prepare(ddpm, optim, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1tWKfF6u0Q9",
    "outputId": "e2b04082-b82d-481f-e92e-7089fa111b23"
   },
   "outputs": [],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wxKbzEa1y-e"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "for x, _ in dataloader:\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    ddpm(x)\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    eps = ddpm.create_batch_random_qr(1, (1, 28, 28))\n",
    "    filled_tensor = torch.full(eps.size(), -0.5)\n",
    "    eps += filled_tensor\n",
    "    save_image(eps, \"random_qr.png\")\n",
    "\n",
    "    t = torch.randint(0, ddpm.n_T, (x.shape[0],), device=x.device)\n",
    "\n",
    "    sigma_t = ddpm.sigma_t[t]\n",
    "\n",
    "    # Transform each image in the batch x with the corresponding QR code pattern\n",
    "    transformed_x = ddpm.apply_qr_transformation(x, sigma_t)\n",
    "\n",
    "    save_image(transformed_x, \"./contents/transformed_image.png\", nrow=16)\n",
    "\n",
    "    xh = ddpm.sample(16, (1, 28, 28), accelerator.device)\n",
    "    # Save samples to `./contents` directory\n",
    "    save_image(xh, \"./contents/grid_image.png\", nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLvihb9Mu0RA"
   },
   "outputs": [],
   "source": [
    "def reformat(dataset):\n",
    "    processed_imgs = []\n",
    "    for img in dataset:\n",
    "        resized_img = TF.resize(img, size=(299, 299))\n",
    "        # Convert grayscale to RGB by repeating the grayscale channel 3 times\n",
    "        rgb_img = resized_img.repeat(1, 3, 1, 1)\n",
    "        processed_imgs.append(rgb_img)\n",
    "    # Stack all processed images into a single tensor\n",
    "    return torch.cat(processed_imgs, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN6pQeg2u0RA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# 'dataloader' is the DataLoader for MNIST\n",
    "real_mnist, _ = next(iter(dataloader))\n",
    "\n",
    "# If necessary, move to the same device as your generated images\n",
    "real_mnist = real_mnist.to(accelerator.device)\n",
    "\n",
    "processed_mnist = []\n",
    "for mnist in real_mnist:\n",
    "    # Resize image to 299x299\n",
    "    resized_mnist = TF.resize(mnist, size=(299, 299))\n",
    "    # Convert grayscale to RGB by repeating the grayscale channel 3 times\n",
    "    rgb_mnist = resized_mnist.repeat(1, 3, 1, 1)\n",
    "    processed_mnist.append(rgb_mnist)\n",
    "# Stack all processed images into a single tensor\n",
    "real_images = torch.cat(processed_mnist, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgdpf_skfzkH"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "def calculate_fid_custom(real_images, gen_images):\n",
    "    # Load the pretrained Inception v3 model\n",
    "    inception_model = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "    inception_model.eval()\n",
    "    # Modify the model to return features from an intermediate layer\n",
    "    inception_model.fc = torch.nn.Identity()\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        gen_features = inception_model(gen_images)\n",
    "        gen_features = gen_features.cpu().numpy()\n",
    "        real_features = inception_model(real_images)\n",
    "        real_features = real_features.cpu().numpy()\n",
    "\n",
    "    # Calculate mean and covariance of features\n",
    "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(\n",
    "        real_features, rowvar=False\n",
    "    )\n",
    "    mu_gen, sigma_gen = np.mean(gen_features, axis=0), np.cov(\n",
    "        gen_features, rowvar=False\n",
    "    )\n",
    "\n",
    "    # Calculate the FID score\n",
    "    ssdiff = np.sum((mu_real - mu_gen) ** 2.0)\n",
    "    covmean = sqrtm(sigma_real.dot(sigma_gen))\n",
    "\n",
    "    # Check for imaginary numbers in covmean and eliminate them\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma_real + sigma_gen - 2.0 * covmean)\n",
    "\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLiE8x-c1y-e",
    "outputId": "2bb2ee15-fc93-425a-aaf7-ad95b0eed7c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "losses = []\n",
    "fid_scores = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    ddpm.train()\n",
    "\n",
    "    pbar = tqdm(dataloader)  # Wrap our loop with a visual progress bar\n",
    "    for x, _ in pbar:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = ddpm(x)\n",
    "\n",
    "        # loss.backward()\n",
    "        # ^Technically should be `accelerator.backward(loss)` but not necessary for local training\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        avg_loss = np.average(losses[min(len(losses) - 100, 0) :])\n",
    "        pbar.set_description(\n",
    "            f\"loss: {avg_loss:.3g}\"\n",
    "        )  # Show running average of loss in progress bar\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    # Step the scheduler at the end of each epoch\n",
    "    scheduler.step(avg_loss)\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        xh = ddpm.sample(\n",
    "            16, (1, 28, 28), accelerator.device\n",
    "        )  # Can get device explicitly with `accelerator.device`\n",
    "        save_image(xh, f\"./contents/ddpm_sample_{i:04d}.png\")\n",
    "        # Save samples to `./contents` directory\n",
    "\n",
    "        fid = calculate_fid_custom(real_images[:16].cpu(), reformat(xh).cpu())\n",
    "        fid_scores.append(fid)\n",
    "\n",
    "        # save model\n",
    "        torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n",
    "        torch.save(losses, \"./ddpm_losses.pt\")\n",
    "        torch.save(fid_scores, \"./fid_scores.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC469ckjGCp1"
   },
   "outputs": [],
   "source": [
    "ddpm1 = DDPM(gt=gt, n_T=1000)\n",
    "ddpm1.load_state_dict(torch.load(\"../pure_qr.pth\", map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xh = ddpm.sample(16, (1, 28, 28), device=accelerator.device)\n",
    "save_image(xh, \"./contents/ddpm_sample_final.png\")\n",
    "plt.imshow(xh[0, 0].cpu().detach().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
