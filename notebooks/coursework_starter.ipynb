{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOqtixLa1y-b"
   },
   "source": [
    "The following example notebook implements standard diffusion\n",
    "with a simple CNN model to generate realistic MNIST digits.\n",
    "\n",
    "This is a modified implementation of `minDiffusion`\n",
    "which implements [DDPM](https://arxiv.org/abs/2006.11239)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this example notebook,\n",
    "install requirements as in `requirements.txt` (for example, `pip install -r requirements.txt`).\n",
    "You may also wish to follow system-dependent PyTorch instructions\n",
    "[here](https://pytorch.org/) to install accelerated\n",
    "versions of PyTorch, but note they are not needed\n",
    "(I am testing this on my laptop).\n",
    "\n",
    "If you do use accelerated hardware, make sure that your code\n",
    "is still compatible with CPU-only installs.\n",
    "\n",
    "First, let's create a folder to store example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (2.0.0.post200)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "50FGtZsk1y-b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a DDPM training schedule for use when evaluating\n",
    "and training the diffusion model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MMQ1-BSc1y-c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Returns pre-computed schedules for DDPM sampling with a linear noise schedule.\"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    alpha_t = torch.exp(torch.cumsum(torch.log(1 - beta_t), dim=0))  # Cumprod in log-space (better precision)\n",
    "\n",
    "    return {\"beta_t\": beta_t, \"alpha_t\": alpha_t}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a simple 2D convolutional neural network. This network\n",
    "is essentially going to try to estimate the diffusion process --- we\n",
    "can then use this network to generate realistic images.\n",
    "\n",
    "First, we create a single CNN block which we will stack to create the\n",
    "full network. We use `LayerNorm` for stable training and no batch dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "d16i_bcV1y-d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        *,\n",
    "        expected_shape,\n",
    "        act=nn.GELU,\n",
    "        kernel_size=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "                        nn.LayerNorm((out_channels, *expected_shape)),\n",
    "            act()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the full CNN model, which is a stack of these blocks\n",
    "according to the `n_hidden` tuple, which specifies the number of\n",
    "channels at each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "ZSvzdt1f1y-d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        expected_shape=(28, 28),\n",
    "        n_hidden=(64, 128, 64),\n",
    "        kernel_size=7,\n",
    "        last_kernel_size=3,\n",
    "        time_embeddings=16,\n",
    "        act=nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        last = in_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for hidden in n_hidden:\n",
    "            self.blocks.append(\n",
    "                CNNBlock(\n",
    "                    last,\n",
    "                    hidden,\n",
    "                    expected_shape=expected_shape,\n",
    "                    kernel_size=kernel_size,\n",
    "                    act=act,\n",
    "                )\n",
    "            )\n",
    "            last = hidden\n",
    "\n",
    "        # The final layer, we use a regular Conv2d to get the\n",
    "        # correct scale and shape (and avoid applying the activation)\n",
    "        self.blocks.append(\n",
    "            nn.Conv2d(\n",
    "                last,\n",
    "                in_channels,\n",
    "                last_kernel_size,\n",
    "                padding=last_kernel_size // 2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## This part is literally just to put the single scalar \"t\" into the CNN\n",
    "        ## in a nice, high-dimensional way:\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_embeddings * 2, 128), act(),\n",
    "            nn.Linear(128, 128), act(),\n",
    "            nn.Linear(128, 128), act(),\n",
    "            nn.Linear(128, n_hidden[0]),\n",
    "        )\n",
    "        frequencies = torch.tensor(\n",
    "            [0] + [2 * np.pi * 1.5**i for i in range(time_embeddings - 1)]\n",
    "        )\n",
    "        self.register_buffer(\"frequencies\", frequencies)\n",
    "\n",
    "    def time_encoding(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        phases = torch.concat(\n",
    "            (\n",
    "                torch.sin(t[:, None] * self.frequencies[None, :]),\n",
    "                torch.cos(t[:, None] * self.frequencies[None, :]) - 1,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return self.time_embed(phases)[:, :, None, None]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # Shapes of input:\n",
    "        #    x: (batch, chan, height, width)\n",
    "        #    t: (batch,)\n",
    "\n",
    "        embed = self.blocks[0](x)\n",
    "        # ^ (batch, n_hidden[0], height, width)\n",
    "\n",
    "        # Add information about time along the diffusion process\n",
    "        #  (Providing this information by superimposing in latent space)\n",
    "        embed += self.time_encoding(t)\n",
    "        #         ^ (batch, n_hidden[0], 1, 1) - thus, broadcasting\n",
    "        #           to the entire spatial domain\n",
    "\n",
    "        for block in self.blocks[1:]:\n",
    "            embed = block(embed)\n",
    "\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the actual diffusion model, which specifies the training\n",
    "schedule, takes an arbitrary model for estimating the\n",
    "diffusion process (such as the CNN above),\n",
    "and computes the corresponding loss (as well as generating samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "pCZe8Q651y-d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gt,\n",
    "        betas: Tuple[float, float],\n",
    "        n_T: int,\n",
    "        criterion: nn.Module = nn.MSELoss(),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.gt = gt\n",
    "\n",
    "        noise_schedule = ddpm_schedules(betas[0], betas[1], n_T)\n",
    "\n",
    "        # `register_buffer` will track these tensors for device placement, but\n",
    "        # not store them as model parameters. This is useful for constants.\n",
    "        self.register_buffer(\"beta_t\", noise_schedule[\"beta_t\"])\n",
    "        self.beta_t  # Exists! Set by register_buffer\n",
    "        self.register_buffer(\"alpha_t\", noise_schedule[\"alpha_t\"])\n",
    "        self.alpha_t\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Algorithm 18.1 in Prince\"\"\"\n",
    "\n",
    "        t = torch.randint(1, self.n_T, (x.shape[0],), device=x.device)\n",
    "        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "        alpha_t = self.alpha_t[t, None, None, None]  # Get right shape for broadcasting\n",
    "\n",
    "        z_t = torch.sqrt(alpha_t) * x + torch.sqrt(1 - alpha_t) * eps\n",
    "        # This is the z_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this z_t. Loss is what we return.\n",
    "\n",
    "        return self.criterion(eps, self.gt(z_t, t / self.n_T))\n",
    "\n",
    "    def sample(self, n_sample: int, size, device) -> torch.Tensor:\n",
    "        \"\"\"Algorithm 18.2 in Prince\"\"\"\n",
    "\n",
    "        _one = torch.ones(n_sample, device=device)\n",
    "        z_t = torch.randn(n_sample, *size, device=device)\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            alpha_t = self.alpha_t[i]\n",
    "            beta_t = self.beta_t[i]\n",
    "\n",
    "            # First line of loop:\n",
    "            z_t -= (beta_t / torch.sqrt(1 - alpha_t)) * self.gt(z_t, (i/self.n_T) * _one)\n",
    "            z_t /= torch.sqrt(1 - beta_t)\n",
    "\n",
    "            if i > 1:\n",
    "                # Last line of loop:\n",
    "                z_t += torch.sqrt(beta_t) * torch.randn_like(z_t)\n",
    "            # (We don't add noise at the final step - i.e., the last line of the algorithm)\n",
    "\n",
    "        return z_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run this on MNIST. We perform some basic preprocessing, and set up the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "a6jMrCRa1y-d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))])\n",
    "dataset = MNIST(\"./data\", train=True, download=False, transform=tf)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our model with a given choice of hidden layers and activation function. We also choose a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "-6ApENps1y-d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt = CNN(in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 32, 16), act=nn.GELU)\n",
    "# For testing: (16, 32, 32, 16)\n",
    "# For more capacity (for example): (64, 128, 256, 128, 64)\n",
    "ddpm = DDPM(gt=gt, betas=(1e-4, 0.02), n_T=1000)\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set up a GPU if we have one, which is done below.\n",
    "\n",
    "Here, we use HuggingFace's `accelerate` library, which abstracts away all the `.to(device)` calls for us.\n",
    "This lets us focus on the model itself rather than data movement.\n",
    "It also does a few other tricks to speed up calculations.\n",
    "\n",
    "PyTorch Lightning, which we discussed during the course, is another option that also handles a lot more, but is a bit heavyweight.\n",
    "`accelerate` is a simpler option closer to raw PyTorch.\n",
    "However, if you prefer, you could choose to use Lightning for the coursework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.336, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "# We wrap our model, optimizer, and dataloaders with `accelerator.prepare`,\n",
    "# which lets HuggingFace's Accelerate handle the device placement and gradient accumulation.\n",
    "ddpm, optim, dataloader = accelerator.prepare(ddpm, optim, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just make sure this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "8wxKbzEa1y-e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x, _ in dataloader:\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    ddpm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reformat(dataset):\n",
    "    processed_imgs = []\n",
    "    for img in dataset:\n",
    "        resized_img = TF.resize(img, size=(299, 299))\n",
    "        # Convert grayscale to RGB by repeating the grayscale channel 3 times\n",
    "        rgb_img = resized_img.repeat(1, 3, 1, 1)\n",
    "        processed_imgs.append(rgb_img)\n",
    "    # Stack all processed images into a single tensor\n",
    "    return torch.cat(processed_imgs, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 299, 299])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train it. You can exit early by interrupting the kernel. Images\n",
    "are saved to the `contents` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLiE8x-c1y-e",
    "outputId": "a9f81c32-96c2-4e3b-cee9-fd2d2d4e316c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.31it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.53it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.19it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.45it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.41it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.43it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.61it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.51it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.14it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.62it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.05it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.48it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.63it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.37it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.28it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.46it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 37.43it/s]\n",
      "loss: 0.0173: 100%|██████████| 468/468 [00:12<00:00, 37.31it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 37.16it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 37.10it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 37.17it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 37.02it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.78it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.69it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.73it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.49it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.46it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 37.08it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.94it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.71it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.55it/s]\n",
      "loss: 0.0172: 100%|██████████| 468/468 [00:12<00:00, 36.51it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:12<00:00, 36.77it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:12<00:00, 36.88it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:12<00:00, 36.24it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:12<00:00, 36.54it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.66it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:12<00:00, 36.20it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.54it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.95it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.74it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.71it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.63it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.51it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.68it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.18it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.16it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.52it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.09it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 35.03it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 34.77it/s]\n",
      "loss: 0.0171: 100%|██████████| 468/468 [00:13<00:00, 34.68it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.69it/s] \n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.63it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.82it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.88it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.22it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.43it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.34it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.54it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.35it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 37.09it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 35.05it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.48it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.01it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 35.84it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.50it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 34.99it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.16it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.39it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 35.45it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:13<00:00, 35.75it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.26it/s]\n",
      "loss: 0.017: 100%|██████████| 468/468 [00:12<00:00, 36.05it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.81it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 34.75it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.23it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.94it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.44it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:12<00:00, 36.24it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.90it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:12<00:00, 36.07it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.75it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.82it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.67it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.80it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.76it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.04it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.97it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.55it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.46it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.48it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.57it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.21it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.31it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 35.19it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 34.94it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 34.59it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 34.84it/s]\n",
      "loss: 0.0169: 100%|██████████| 468/468 [00:13<00:00, 34.73it/s]\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "losses = []\n",
    "fid_scores = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    ddpm.train()\n",
    "\n",
    "    pbar = tqdm(dataloader)  # Wrap our loop with a visual progress bar\n",
    "    for x, _ in pbar:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = ddpm(x)\n",
    "\n",
    "        # loss.backward()\n",
    "        # ^Technically should be `accelerator.backward(loss)` but not necessary for local training\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        avg_loss = np.average(losses[min(len(losses)-100, 0):])\n",
    "        pbar.set_description(f\"loss: {avg_loss:.3g}\")  # Show running average of loss in progress bar\n",
    "\n",
    "        optim.step()\n",
    "    \n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        xh = ddpm.sample(128, (1, 28, 28), accelerator.device)  # Can get device explicitly with `accelerator.device`\n",
    "        grid = make_grid(xh, nrow=4)\n",
    "        fid = calculate_fid_given_samples([real_images, reformat(xh)], device = device)\n",
    "        fid_scores.append(fid)\n",
    "        # Save samples to `./contents` directory\n",
    "        # save_image(grid, f\"./contents/ddpm_sample_{i:04d}.png\")\n",
    "\n",
    "        # save model\n",
    "        torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n",
    "        torch.save(losses, \"./ddpm_losses.pt\")\n",
    "        torch.save(fid_scores, \"./fid_scores.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ddpm.load_state_dict(torch.load('starter.pth', map_location=torch.device(device)))\n",
    "losses = torch.load('starter_losses.pt', map_location=torch.device(device))\n",
    "epoch_losses = []\n",
    "for i in range(0, len(losses), 467):\n",
    "    # print(losses[i])\n",
    "    epoch_losses.append(losses[i])\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(fid_scores)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('FID score')\n",
    "plt.title('FID Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception and FID score calulated using functions from Bansal et al. 2022\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "try:\n",
    "    from torchvision.models.utils import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "# Inception weights ported to Pytorch from\n",
    "# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
    "FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'  # noqa: E501\n",
    "\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,   # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
    "                 resize_input=True,\n",
    "                 normalize_input=True,\n",
    "                 requires_grad=False,\n",
    "                 use_fid_inception=True):\n",
    "        \"\"\"Build pretrained InceptionV3\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_blocks : list of int\n",
    "            Indices of blocks to return features of. Possible values are:\n",
    "                - 0: corresponds to output of first max pooling\n",
    "                - 1: corresponds to output of second max pooling\n",
    "                - 2: corresponds to output which is fed to aux classifier\n",
    "                - 3: corresponds to output of final average pooling\n",
    "        resize_input : bool\n",
    "            If true, bilinearly resizes input to width and height 299 before\n",
    "            feeding input to model. As the network without fully connected\n",
    "            layers is fully convolutional, it should be able to handle inputs\n",
    "            of arbitrary size, so resizing might not be strictly needed\n",
    "        normalize_input : bool\n",
    "            If true, scales the input from range (0, 1) to the range the\n",
    "            pretrained Inception network expects, namely (-1, 1)\n",
    "        requires_grad : bool\n",
    "            If true, parameters of the model require gradients. Possibly useful\n",
    "            for finetuning the network\n",
    "        use_fid_inception : bool\n",
    "            If true, uses the pretrained Inception model used in Tensorflow's\n",
    "            FID implementation. If false, uses the pretrained Inception model\n",
    "            available in torchvision. The FID Inception model has different\n",
    "            weights and a slightly different structure from torchvision's\n",
    "            Inception model. If you want to compute FID scores, you are\n",
    "            strongly advised to set this parameter to true to get comparable\n",
    "            results.\n",
    "        \"\"\"\n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \\\n",
    "            'Last possible output block index is 3'\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        if use_fid_inception:\n",
    "            inception = fid_inception_v3()\n",
    "        else:\n",
    "            inception = _inception_v3(pretrained=True)\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x,\n",
    "                              size=(299, 299),\n",
    "                              mode='bilinear',\n",
    "                              align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp\n",
    "\n",
    "\n",
    "def _inception_v3(*args, **kwargs):\n",
    "    \"\"\"Wraps `torchvision.models.inception_v3`\n",
    "\n",
    "    Skips default weight inititialization if supported by torchvision version.\n",
    "    See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        version = tuple(map(int, torchvision.__version__.split('.')[:2]))\n",
    "    except ValueError:\n",
    "        # Just a caution against weird version strings\n",
    "        version = (0,)\n",
    "\n",
    "    if version >= (0, 6):\n",
    "        kwargs['init_weights'] = False\n",
    "\n",
    "    return torchvision.models.inception_v3(*args, **kwargs)\n",
    "\n",
    "\n",
    "def fid_inception_v3():\n",
    "    \"\"\"Build pretrained Inception model for FID computation\n",
    "\n",
    "    The Inception model for FID computation uses a different set of weights\n",
    "    and has a slightly different structure than torchvision's Inception.\n",
    "\n",
    "    This method first constructs torchvision's Inception and then patches the\n",
    "    necessary parts that are different in the FID Inception model.\n",
    "    \"\"\"\n",
    "    inception = _inception_v3(num_classes=1008,\n",
    "                              aux_logits=False,\n",
    "                              pretrained=False)\n",
    "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
    "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
    "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
    "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
    "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
    "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
    "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
    "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
    "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
    "\n",
    "    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n",
    "    inception.load_state_dict(state_dict)\n",
    "    return inception\n",
    "\n",
    "\n",
    "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
    "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
    "                                   count_include_pad=False)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
    "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
    "                                   count_include_pad=False)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
    "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
    "                                   count_include_pad=False)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
    "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        # Patch: The FID Inception model uses max pooling instead of average\n",
    "        # pooling. This is likely an error in this specific Inception\n",
    "        # implementation, as other Inception models use average pooling here\n",
    "        # (which matches the description in the paper).\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Calculates the Frechet Inception Distance (FID) to evalulate GANs\n",
    "\n",
    "The FID metric calculates the distance between two distributions of images.\n",
    "Typically, we have summary statistics (mean & covariance matrix) of one\n",
    "of these distributions, while the 2nd distribution is given by a GAN.\n",
    "\n",
    "When run as a stand-alone program, it compares the distribution of\n",
    "images that are stored as PNG/JPEG at a specified location with a\n",
    "distribution given by summary statistics (in pickle format).\n",
    "\n",
    "The FID is calculated by assuming that X_1 and X_2 are the activations of\n",
    "the pool_3 layer of the inception net for generated samples and real world\n",
    "samples respectively.\n",
    "\n",
    "See --help to see further details.\n",
    "\n",
    "Code apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\n",
    "of Tensorflow\n",
    "\n",
    "Copyright 2018 Institute of Bioinformatics, JKU Linz\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    # If tqdm is not available, provide a mock version of it\n",
    "    def tqdm(x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "# parser.add_argument('--batch-size', type=int, default=50,\n",
    "#                     help='Batch size to use')\n",
    "# parser.add_argument('--num-workers', type=int,\n",
    "#                     help=('Number of processes to use for data loading. '\n",
    "#                           'Defaults to `min(8, num_cpus)`'))\n",
    "# parser.add_argument('--device', type=str, default=None,\n",
    "#                     help='Device to use. Like cuda, cuda:0 or cpu')\n",
    "# parser.add_argument('--dims', type=int, default=2048,\n",
    "#                     choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),\n",
    "#                     help=('Dimensionality of Inception features to use. '\n",
    "#                           'By default, uses pool3 features'))\n",
    "# parser.add_argument('path', type=str, nargs=2,\n",
    "#                     help=('Paths to the generated images or '\n",
    "#                           'to .npz statistic files'))\n",
    "#\n",
    "IMAGE_EXTENSIONS = {'bmp', 'jpg', 'jpeg', 'pgm', 'png', 'ppm',\n",
    "                    'tif', 'tiff', 'webp'}\n",
    "\n",
    "\n",
    "class ImagePathDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, transforms=None):\n",
    "        self.files = files\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_activations(samples, model, batch_size=50, dims=2048, device='cpu',\n",
    "                    num_workers=1):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : Batch size of images for the model to process at once.\n",
    "                     Make sure that the number of samples is a multiple of\n",
    "                     the batch size, otherwise some samples are ignored. This\n",
    "                     behavior is retained to match the original FID score\n",
    "                     implementation.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- device      : Device to run calculations\n",
    "    -- num_workers : Number of parallel dataloader workers\n",
    "\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, dims) that contains the\n",
    "       activations of the given tensor when feeding inception with the\n",
    "       query tensor.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # samples is a torch array\n",
    "\n",
    "    pred_arr = np.empty((samples.shape[0], dims))\n",
    "    start_idx = 0\n",
    "\n",
    "    n_iters = samples.shape[0] // batch_size # try to make this an int\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        batch = samples[i*batch_size: i*batch_size + batch_size]\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "        if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "        pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "\n",
    "        pred_arr[start_idx:start_idx + pred.shape[0]] = pred\n",
    "\n",
    "        start_idx = start_idx + pred.shape[0]\n",
    "\n",
    "    return pred_arr\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "\n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1   : Numpy array containing the activations of a layer of the\n",
    "               inception net (like returned by the function 'get_predictions')\n",
    "               for generated samples.\n",
    "    -- mu2   : The sample mean over activations, precalculated on an\n",
    "               representative data set.\n",
    "    -- sigma1: The covariance matrix over activations for generated samples.\n",
    "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
    "               representative data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1)\n",
    "            + np.trace(sigma2) - 2 * tr_covmean)\n",
    "\n",
    "def calculate_activation_statistics(samples, model, batch_size=50, dims=2048,\n",
    "                                    device='cpu', num_workers=1):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : The images numpy array is split into batches with\n",
    "                     batch size batch_size. A reasonable batch size\n",
    "                     depends on the hardware.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- device      : Device to run calculations\n",
    "    -- num_workers : Number of parallel dataloader workers\n",
    "\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "               the inception model.\n",
    "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "               the inception model.\n",
    "    \"\"\"\n",
    "    act = get_activations(samples, model, batch_size, dims, device, num_workers)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def compute_statistics_of_samples(samples, model, batch_size, dims, device,\n",
    "                               num_workers=1):\n",
    "\n",
    "    m, s = calculate_activation_statistics(samples, model, batch_size,\n",
    "                                           dims, device, num_workers)\n",
    "\n",
    "    return m, s\n",
    "\n",
    "def calculate_fid_given_samples(samples, batch_size=50, device='cuda:0', dims=2048, num_workers=1):\n",
    "    \"\"\"Calculates the FID of two sample collection\"\"\"\n",
    "\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "\n",
    "    model = InceptionV3([block_idx]).to(device)\n",
    "\n",
    "    m1, s1 = compute_statistics_of_samples(samples[0], model, batch_size,\n",
    "                                        dims, device, num_workers)\n",
    "    m2, s2 = compute_statistics_of_samples(samples[1], model, batch_size,\n",
    "                                        dims, device, num_workers)\n",
    "    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
    "\n",
    "    return fid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ddpm.to(device)\n",
    "ddpm.load_state_dict(torch.load('starter.pth', map_location=torch.device(device)))\n",
    "ddpm.eval() #put it in eval mode\n",
    "with torch.no_grad(): # Making sure, since it is not needed\n",
    "        generated_dataset= ddpm.sample(128, (1, 28, 28), device)\n",
    "        # Should be a large number of samples to calculte the inception score\n",
    "        # Same number as the batch size for MNIST dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fid score:  80.8719640000154\n"
     ]
    }
   ],
   "source": [
    "# 'dataloader' is the DataLoader for MNIST\n",
    "real_mnist, _ = next(iter(dataloader))\n",
    "\n",
    "# If necessary, move to the same device as your generated images\n",
    "real_mnist = real_mnist.to(device)\n",
    "\n",
    "processed_mnist = []\n",
    "for mnist in real_mnist:\n",
    "    # Resize image to 299x299\n",
    "    resized_mnist = TF.resize(mnist, size=(299, 299))\n",
    "    # Convert grayscale to RGB by repeating the grayscale channel 3 times\n",
    "    rgb_mnist = resized_mnist.repeat(1, 3, 1, 1)\n",
    "    processed_mnist.append(rgb_mnist)\n",
    "# Stack all processed images into a single tensor\n",
    "real_images = torch.cat(processed_mnist, dim=0)\n",
    "processed_imgs = []\n",
    "for img in generated_dataset:\n",
    "    resized_img = TF.resize(img, size=(299, 299))\n",
    "    # Convert grayscale to RGB by repeating the grayscale channel 3 times\n",
    "    rgb_img = resized_img.repeat(1, 3, 1, 1)\n",
    "    processed_imgs.append(rgb_img)\n",
    "# Stack all processed images into a single tensor\n",
    "imgs = torch.cat(processed_imgs, dim=0)\n",
    "\n",
    "real_images, generated_images = real_images.cpu(), imgs.cpu()\n",
    "\n",
    "fid = calculate_fid_given_samples([real_images, generated_images],device = device)\n",
    "print(\"Fid score: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4785e-12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate using torchmetrics\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature= 2048)\n",
    "fid.reset()\n",
    "fid.update(real_images.cpu().to(dtype=torch.uint8), real=True)\n",
    "fid.update(generated_images.to(dtype=torch.uint8), real=False)\n",
    "fid.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(0.))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = InceptionScore()\n",
    "inception.update(generated_images.to(dtype=torch.uint8))\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score: 68.84274337687255\n"
     ]
    }
   ],
   "source": [
    "# Calculate using my own functions\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.transforms import functional as TF\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(real_images, gen_images):\n",
    "    # Load the pretrained Inception v3 model\n",
    "    inception_model = inception_v3(pretrained=True)\n",
    "    inception_model.eval()\n",
    "    # Modify the model to return features from an intermediate layer\n",
    "    inception_model.fc = torch.nn.Identity()\n",
    "\n",
    "    def preprocess_and_extract_features(images):\n",
    "        processed_images = []\n",
    "        for img in images:\n",
    "            # Resize and convert 1-channel images to 3-channel images\n",
    "            resized_img = TF.resize(img, size=(299, 299))\n",
    "            rgb_img = resized_img.repeat(1, 3, 1, 1)\n",
    "            processed_images.append(rgb_img)\n",
    "\n",
    "        # Stack all processed images into a single tensor\n",
    "        preprocessed_images = torch.cat(processed_images, dim=0)\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = inception_model(preprocessed_images)\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "    # Preprocess and extract features for real and generated images\n",
    "    real_features = preprocess_and_extract_features(real_images)\n",
    "    gen_features = preprocess_and_extract_features(gen_images)\n",
    "\n",
    "    # Calculate mean and covariance of features\n",
    "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(gen_features, axis=0), np.cov(gen_features, rowvar=False)\n",
    "\n",
    "    # Calculate the FID score\n",
    "    ssdiff = np.sum((mu_real - mu_gen) ** 2.0)\n",
    "    covmean = sqrtm(sigma_real.dot(sigma_gen))\n",
    "\n",
    "    # Check for imaginary numbers in covmean and eliminate them\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma_real + sigma_gen - 2.0 * covmean)\n",
    "\n",
    "    return fid\n",
    "images, _ = next(iter(dataloader))\n",
    "fid_score = calculate_fid(images.cpu(),generated_dataset.cpu())\n",
    "print(\"FID Score:\", fid_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
